{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 신경망 복습\n",
    "\n",
    "* 1.1 수학과 파이썬 복습 (pass)\n",
    "* 1.2 신경망의 구조 (pass)\n",
    "* 1.3 신경망의 학습\n",
    "* 1.4 신경망으로 문제를 풀다\n",
    "* 1.5 계산 고속화\n",
    "* 1.6 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 신경망의 학습\n",
    "\n",
    "* 학습되지 않은 신경망은 '좋은 추론'을 해낼 수 없다.\n",
    "* 신경망 학습은 최적의 매개변수 값을 찾는 작업이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 손실 함수\n",
    "\n",
    "* 신경망 학습에서 사용하는 지표는 '손실'이다.\n",
    "* '손실'은 학습 데이터와 신경망이 예측한 결과를 비교하여 얼마나 '나쁜가'를 나타낸다.\n",
    "* 신경망의 손실은 '손실 함수'로 구한다.\n",
    "* 다중 클래스 분류 신경망에서는 '교차 엔트로피 오차'를 사용한다.\n",
    "* 교차 엔트로피 오차는 정답 레이블과 신경망의 출력 간의 오차를 나타낸다.\n",
    "\n",
    "**소프트맥스 함수**\n",
    "$$\n",
    "y_{k} = \\frac{exp(s_{k})}{\\sum_{i=1}^{n}exp(s_{i})}\n",
    "$$\n",
    "\n",
    "* 출력이 총$n$개일 때, $k$번째 출력 $y_{k}$를 구하는 식이다.\n",
    "* $y_{k}$는 $k$번째 클랙스에 해당하는 소프트맥스 함수의 출력이다.\n",
    "* 소프트맥스 함수의 분자는 점수$s_{k}$의 지수 함수이고, 분모는 모든 클래스의 점수의 지수 함수의 합이다.\n",
    "* 소프트맥스 함수의 출력은 0에서 1사이의 실수이며, 출력의 총합은 1이다.\n",
    "\n",
    "**교차 엔트로피 오차**\n",
    "$$\n",
    "L = -\\sum_{k}t_{k}log(y_{k})\n",
    "$$\n",
    "\n",
    "* $t_{k}$는 $k$번째 클래스에 해당하는 정답 레이블이다.\n",
    "* log는 네이피어 상수 $e$를 밑으로 하는 자연로그이다.\n",
    "* 정답 레이블은 원-핫 인코딩으로 표현한다.\n",
    "\n",
    "**미니배치를 고려한 교차 엔트로피 오차**\n",
    "$$\n",
    "L = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}log(y_{nk})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 미분과 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.3 연쇄 법칙\n",
    "\n",
    "* 신경망의 기울기는 '오차역전파법'으로 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.4 계산 그래프\n",
    "\n",
    "* 계산 그래프는 계산 과정을 그래프로 나타낸 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 기울기 도출과 역전파 구현\n",
    "\n",
    "**Sigmoid 계층**\n",
    "\n",
    "시그모이드의 함수는 $$y = \\frac{1}{1+exp(-x)}$$\n",
    "시그모이드의 미분은 $$\\frac{dy}{dx} = y(1-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.out = None\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = 1/(1+np.exp(-x))\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * (1.0 - self.out) * self.out\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73105858 0.37754067]\n",
      " [0.11920292 0.95257413]]\n",
      "[[0.19661193 0.23500371]\n",
      " [0.10499359 0.04517666]]\n",
      "(2, 2)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# test Sigmod class\n",
    "\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "sigmoid = Sigmoid()\n",
    "print(sigmoid.forward(x))\n",
    "sigmoid.out = sigmoid.forward(x)\n",
    "print(sigmoid.backward(1))\n",
    "print(sigmoid.backward(1).shape)\n",
    "print(sigmoid.backward(1).dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affine 계층**\n",
    "\n",
    "Affine계층의 순전파는 $y=xW+b$이다. `np.matmul(x, W) + b`로 구현할 수 있다.\n",
    "\n",
    "Affine계층의 역전파는 $dy = dxW^{T}$, $dW = x^{T}dy$, $db = \\sum_{i}dy_{i}$이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "  def __init__(self, W, b):\n",
    "    self.params = [W, b]\n",
    "    self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "    self.x = None\n",
    "    \n",
    "  def forward(self, x):\n",
    "    W, b = self.params\n",
    "    out = np.matmul(x, W) + b\n",
    "    self.x = x\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    W, b = self.params\n",
    "    dx = np.matmul(dout, W.T)\n",
    "    dW = np.matmul(self.x.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    \n",
    "    self.grads[0][...] = dW\n",
    "    self.grads[1][...] = db\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 20 24]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m affine \u001b[38;5;241m=\u001b[39m Affine(W, b)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(affine\u001b[38;5;241m.\u001b[39mforward(x))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maffine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(affine\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(affine\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mAffine.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout):\n\u001b[1;32m     14\u001b[0m   W, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m---> 15\u001b[0m   dx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   dW \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mT, dout)\n\u001b[1;32m     17\u001b[0m   db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dout, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# test Affine class\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([7, 8, 9])\n",
    "x = np.array([[1, 2]])\n",
    "affine = Affine(W, b)\n",
    "print(affine.forward(x))\n",
    "print(affine.backward(1))\n",
    "print(affine.backward(1).shape)\n",
    "print(affine.backward(1).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
