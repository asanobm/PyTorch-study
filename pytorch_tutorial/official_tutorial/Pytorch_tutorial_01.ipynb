{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 빠른시작\n",
    "[LINK](https://tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html)\n",
    "\n",
    "이번 장에서는 기계학습의 일반적인 작업들을 위한 API를 통해 실행됩니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "845f4c3f78fb707"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.143292Z",
     "start_time": "2024-01-20T06:37:33.100653Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 작업하기\n",
    "\n",
    "파이토치(Pytorch)에는 *데이터 작업을 위한 기본 요소*두가지인 `torch.utils.DataLoader`와 `torch.utils.Dataset`이 있습니다. `Dataset`은 샘플과 정답(label)을 저장하고, `DataLoader`는 `Dataset`을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5581e8cc91a92a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch는 *TorchText*, *TorchVision*, *TorchAudio*, *TorchScatter*와 같이 도메인 특화 라이브러리를 위한 몇가지 도구를 제공합니다. 이 튜토리얼에서는 *TorchVision*의 몇가지 기능을 사용합니다.\n",
    "\n",
    "`torchvision.datasets`모듈은 CIFAR, COCO등과 같은 다양한 실제 비전(vision)데이터에 대한 `Dataset`을 포함하고 있습니다. 이 튜토리얼에서는 FasionMNIST 데이터셋을 사용합니다. 모든 TorchVision `Dataset`들은 샘플과 정답을 각각 변경하기 위한 `transform`과 `target_transform`을 인자로 받을 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa3ebfda27ab98b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.155969Z",
     "start_time": "2024-01-20T06:37:37.142685Z"
    }
   },
   "id": "7e8efb4975e2f2cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Dataset`을 `DataLoader`의 인자로 전달합니다. 이는 `Dataset`을 순회 가능한 객체(iterable)로 감싸고, 자동화된 배치(batch), 샘플링(sampling), 섞기(shuffle), 병렬 처리를 지원합니다. 여기서 배치 크기(batch size)는 튜토리얼의 편의를 위해 작은 값으로 설정합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16141fbf9cec2f21"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.162988Z",
     "start_time": "2024-01-20T06:37:37.156682Z"
    }
   },
   "id": "e3834e2d0acb9655"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 만들기\n",
    "\n",
    "PyTorch에서 신경망 모델은 *nn.Module**을 상속받는 클래스(class)를 생성하여 정의합니다. `__init__`함수에서 신경망의 계층(layer)들을 정의하고 `forward`함수에서 신경망에 데이터를 어떻게 전달할지 지정합니다. 가능한 경우 GPU또는 MPS로 신경망을 이동시켜 연산을 가속(accelerate)할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a2ec78b62369c9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Using {} device\".format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.168453Z",
     "start_time": "2024-01-20T06:37:37.163419Z"
    }
   },
   "id": "a1188e61ac54dadd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.196511Z",
     "start_time": "2024-01-20T06:37:37.167824Z"
    }
   },
   "id": "258a11687a3cecea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 매개변수 최적화하기\n",
    "\n",
    "모델을 학습하려면 손실 함수(loss function)와 옵티마이저(optimizer)가 필요합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28fc54f6cb78aa38"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.196873Z",
     "start_time": "2024-01-20T06:37:37.172775Z"
    }
   },
   "id": "b7a6eedb5a348faf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "각 학습 단계(training loop)에서 모델은 (배치(batch)로 제공되는)학습 데이터셋에 대한 예측을 수행하고, 예측 오류를 역전파하여 모델의 매개변수를 조정합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1239edf5f529beb1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch & 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.197199Z",
     "start_time": "2024-01-20T06:37:37.175855Z"
    }
   },
   "id": "6768afe99e75faf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델이 학습하고 있는지를 확인하기 위해 테스트 데이터셋으로 모델의 성능을 확인합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9191a6a26e6d2d2c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:37.197852Z",
     "start_time": "2024-01-20T06:37:37.178674Z"
    }
   },
   "id": "cb59a6842540a3d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 단계는 여러번의 반복단계(epoch)로 이루어집니다. 각 epoch에서는 모델은 더 나은 예측을 하기 위해 매개변수를 학습합니다. 각 `epoch`마다 모델의 정확도(accuracy)와 손실(loss)을 출력합니다. `epoch`마다 정확도가 증가하고 손실이 감소하는 것을 보려고 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cd0300443390c58"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302672 [    0/60000]\n",
      "loss: 2.304448 [   64/60000]\n",
      "loss: 2.294259 [  128/60000]\n",
      "loss: 2.295219 [  192/60000]\n",
      "loss: 2.299822 [  512/60000]\n",
      "loss: 2.296094 [  576/60000]\n",
      "loss: 2.297063 [  640/60000]\n",
      "loss: 2.295778 [  704/60000]\n",
      "loss: 2.303056 [ 1024/60000]\n",
      "loss: 2.306709 [ 1088/60000]\n",
      "loss: 2.296635 [ 1152/60000]\n",
      "loss: 2.298655 [ 1216/60000]\n",
      "loss: 2.302291 [ 1536/60000]\n",
      "loss: 2.304717 [ 1600/60000]\n",
      "loss: 2.289470 [ 1664/60000]\n",
      "loss: 2.295907 [ 1728/60000]\n",
      "loss: 2.299078 [ 8192/60000]\n",
      "loss: 2.288045 [ 8256/60000]\n",
      "loss: 2.287491 [ 8320/60000]\n",
      "loss: 2.301056 [ 8384/60000]\n",
      "loss: 2.292219 [ 8704/60000]\n",
      "loss: 2.291851 [ 8768/60000]\n",
      "loss: 2.288779 [ 8832/60000]\n",
      "loss: 2.283998 [ 8896/60000]\n",
      "loss: 2.280465 [ 9216/60000]\n",
      "loss: 2.288601 [ 9280/60000]\n",
      "loss: 2.276414 [ 9344/60000]\n",
      "loss: 2.296964 [ 9408/60000]\n",
      "loss: 2.286409 [ 9728/60000]\n",
      "loss: 2.289372 [ 9792/60000]\n",
      "loss: 2.287019 [ 9856/60000]\n",
      "loss: 2.297654 [ 9920/60000]\n",
      "loss: 2.274833 [16384/60000]\n",
      "loss: 2.274919 [16448/60000]\n",
      "loss: 2.269112 [16512/60000]\n",
      "loss: 2.279617 [16576/60000]\n",
      "loss: 2.281737 [16896/60000]\n",
      "loss: 2.278801 [16960/60000]\n",
      "loss: 2.280973 [17024/60000]\n",
      "loss: 2.276112 [17088/60000]\n",
      "loss: 2.272550 [17408/60000]\n",
      "loss: 2.268533 [17472/60000]\n",
      "loss: 2.271103 [17536/60000]\n",
      "loss: 2.287270 [17600/60000]\n",
      "loss: 2.278891 [17920/60000]\n",
      "loss: 2.268746 [17984/60000]\n",
      "loss: 2.278323 [18048/60000]\n",
      "loss: 2.288656 [18112/60000]\n",
      "loss: 2.258110 [24576/60000]\n",
      "loss: 2.270057 [24640/60000]\n",
      "loss: 2.271906 [24704/60000]\n",
      "loss: 2.268605 [24768/60000]\n",
      "loss: 2.265636 [25088/60000]\n",
      "loss: 2.266964 [25152/60000]\n",
      "loss: 2.265255 [25216/60000]\n",
      "loss: 2.254264 [25280/60000]\n",
      "loss: 2.265663 [25600/60000]\n",
      "loss: 2.247184 [25664/60000]\n",
      "loss: 2.273623 [25728/60000]\n",
      "loss: 2.260633 [25792/60000]\n",
      "loss: 2.278103 [26112/60000]\n",
      "loss: 2.268655 [26176/60000]\n",
      "loss: 2.278566 [26240/60000]\n",
      "loss: 2.283612 [26304/60000]\n",
      "loss: 2.260481 [32768/60000]\n",
      "loss: 2.255491 [32832/60000]\n",
      "loss: 2.259729 [32896/60000]\n",
      "loss: 2.268970 [32960/60000]\n",
      "loss: 2.254973 [33280/60000]\n",
      "loss: 2.252759 [33344/60000]\n",
      "loss: 2.259457 [33408/60000]\n",
      "loss: 2.250318 [33472/60000]\n",
      "loss: 2.256441 [33792/60000]\n",
      "loss: 2.258021 [33856/60000]\n",
      "loss: 2.270971 [33920/60000]\n",
      "loss: 2.251656 [33984/60000]\n",
      "loss: 2.249953 [34304/60000]\n",
      "loss: 2.255398 [34368/60000]\n",
      "loss: 2.271047 [34432/60000]\n",
      "loss: 2.256074 [34496/60000]\n",
      "loss: 2.264447 [40960/60000]\n",
      "loss: 2.260989 [41024/60000]\n",
      "loss: 2.237125 [41088/60000]\n",
      "loss: 2.229322 [41152/60000]\n",
      "loss: 2.251790 [41472/60000]\n",
      "loss: 2.243930 [41536/60000]\n",
      "loss: 2.247656 [41600/60000]\n",
      "loss: 2.251338 [41664/60000]\n",
      "loss: 2.228918 [41984/60000]\n",
      "loss: 2.235546 [42048/60000]\n",
      "loss: 2.238273 [42112/60000]\n",
      "loss: 2.246406 [42176/60000]\n",
      "loss: 2.238247 [42496/60000]\n",
      "loss: 2.230712 [42560/60000]\n",
      "loss: 2.238997 [42624/60000]\n",
      "loss: 2.267502 [42688/60000]\n",
      "loss: 2.241923 [49152/60000]\n",
      "loss: 2.232727 [49216/60000]\n",
      "loss: 2.253891 [49280/60000]\n",
      "loss: 2.219136 [49344/60000]\n",
      "loss: 2.253678 [49664/60000]\n",
      "loss: 2.226336 [49728/60000]\n",
      "loss: 2.210495 [49792/60000]\n",
      "loss: 2.234300 [49856/60000]\n",
      "loss: 2.251425 [50176/60000]\n",
      "loss: 2.251225 [50240/60000]\n",
      "loss: 2.239766 [50304/60000]\n",
      "loss: 2.217334 [50368/60000]\n",
      "loss: 2.238370 [50688/60000]\n",
      "loss: 2.240662 [50752/60000]\n",
      "loss: 2.250498 [50816/60000]\n",
      "loss: 2.245796 [50880/60000]\n",
      "loss: 2.220665 [57344/60000]\n",
      "loss: 2.219843 [57408/60000]\n",
      "loss: 2.237529 [57472/60000]\n",
      "loss: 2.218779 [57536/60000]\n",
      "loss: 2.208859 [57856/60000]\n",
      "loss: 2.206322 [57920/60000]\n",
      "loss: 2.217422 [57984/60000]\n",
      "loss: 2.225140 [58048/60000]\n",
      "loss: 2.226664 [58368/60000]\n",
      "loss: 2.229298 [58432/60000]\n",
      "loss: 2.243802 [58496/60000]\n",
      "loss: 2.229460 [58560/60000]\n",
      "loss: 2.226908 [58880/60000]\n",
      "loss: 2.229987 [58944/60000]\n",
      "loss: 2.227931 [59008/60000]\n",
      "loss: 2.223428 [59072/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.220831 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.223094 [    0/60000]\n",
      "loss: 2.215615 [   64/60000]\n",
      "loss: 2.223742 [  128/60000]\n",
      "loss: 2.227586 [  192/60000]\n",
      "loss: 2.216849 [  512/60000]\n",
      "loss: 2.226717 [  576/60000]\n",
      "loss: 2.209959 [  640/60000]\n",
      "loss: 2.200180 [  704/60000]\n",
      "loss: 2.211879 [ 1024/60000]\n",
      "loss: 2.235615 [ 1088/60000]\n",
      "loss: 2.211903 [ 1152/60000]\n",
      "loss: 2.220111 [ 1216/60000]\n",
      "loss: 2.201612 [ 1536/60000]\n",
      "loss: 2.214653 [ 1600/60000]\n",
      "loss: 2.189101 [ 1664/60000]\n",
      "loss: 2.220994 [ 1728/60000]\n",
      "loss: 2.232142 [ 8192/60000]\n",
      "loss: 2.193136 [ 8256/60000]\n",
      "loss: 2.199261 [ 8320/60000]\n",
      "loss: 2.226354 [ 8384/60000]\n",
      "loss: 2.210905 [ 8704/60000]\n",
      "loss: 2.208929 [ 8768/60000]\n",
      "loss: 2.230250 [ 8832/60000]\n",
      "loss: 2.202536 [ 8896/60000]\n",
      "loss: 2.186202 [ 9216/60000]\n",
      "loss: 2.193275 [ 9280/60000]\n",
      "loss: 2.178437 [ 9344/60000]\n",
      "loss: 2.238248 [ 9408/60000]\n",
      "loss: 2.204440 [ 9728/60000]\n",
      "loss: 2.209231 [ 9792/60000]\n",
      "loss: 2.187151 [ 9856/60000]\n",
      "loss: 2.224274 [ 9920/60000]\n",
      "loss: 2.195238 [16384/60000]\n",
      "loss: 2.168803 [16448/60000]\n",
      "loss: 2.149010 [16512/60000]\n",
      "loss: 2.175602 [16576/60000]\n",
      "loss: 2.196678 [16896/60000]\n",
      "loss: 2.213329 [16960/60000]\n",
      "loss: 2.200359 [17024/60000]\n",
      "loss: 2.195115 [17088/60000]\n",
      "loss: 2.176099 [17408/60000]\n",
      "loss: 2.180776 [17472/60000]\n",
      "loss: 2.154302 [17536/60000]\n",
      "loss: 2.221979 [17600/60000]\n",
      "loss: 2.200729 [17920/60000]\n",
      "loss: 2.182615 [17984/60000]\n",
      "loss: 2.202460 [18048/60000]\n",
      "loss: 2.226214 [18112/60000]\n",
      "loss: 2.147235 [24576/60000]\n",
      "loss: 2.182719 [24640/60000]\n",
      "loss: 2.182995 [24704/60000]\n",
      "loss: 2.172361 [24768/60000]\n",
      "loss: 2.181552 [25088/60000]\n",
      "loss: 2.172429 [25152/60000]\n",
      "loss: 2.154363 [25216/60000]\n",
      "loss: 2.123733 [25280/60000]\n",
      "loss: 2.158393 [25600/60000]\n",
      "loss: 2.134451 [25664/60000]\n",
      "loss: 2.184113 [25728/60000]\n",
      "loss: 2.160172 [25792/60000]\n",
      "loss: 2.200255 [26112/60000]\n",
      "loss: 2.172065 [26176/60000]\n",
      "loss: 2.206012 [26240/60000]\n",
      "loss: 2.211523 [26304/60000]\n",
      "loss: 2.172589 [32768/60000]\n",
      "loss: 2.154609 [32832/60000]\n",
      "loss: 2.149315 [32896/60000]\n",
      "loss: 2.188055 [32960/60000]\n",
      "loss: 2.139569 [33280/60000]\n",
      "loss: 2.130201 [33344/60000]\n",
      "loss: 2.155477 [33408/60000]\n",
      "loss: 2.139706 [33472/60000]\n",
      "loss: 2.147556 [33792/60000]\n",
      "loss: 2.150348 [33856/60000]\n",
      "loss: 2.195205 [33920/60000]\n",
      "loss: 2.143702 [33984/60000]\n",
      "loss: 2.131685 [34304/60000]\n",
      "loss: 2.159013 [34368/60000]\n",
      "loss: 2.168498 [34432/60000]\n",
      "loss: 2.129495 [34496/60000]\n",
      "loss: 2.168149 [40960/60000]\n",
      "loss: 2.166959 [41024/60000]\n",
      "loss: 2.102335 [41088/60000]\n",
      "loss: 2.082863 [41152/60000]\n",
      "loss: 2.151134 [41472/60000]\n",
      "loss: 2.137231 [41536/60000]\n",
      "loss: 2.134260 [41600/60000]\n",
      "loss: 2.144994 [41664/60000]\n",
      "loss: 2.110080 [41984/60000]\n",
      "loss: 2.122481 [42048/60000]\n",
      "loss: 2.109351 [42112/60000]\n",
      "loss: 2.143149 [42176/60000]\n",
      "loss: 2.125386 [42496/60000]\n",
      "loss: 2.085492 [42560/60000]\n",
      "loss: 2.110492 [42624/60000]\n",
      "loss: 2.174846 [42688/60000]\n",
      "loss: 2.127970 [49152/60000]\n",
      "loss: 2.104657 [49216/60000]\n",
      "loss: 2.153845 [49280/60000]\n",
      "loss: 2.063529 [49344/60000]\n",
      "loss: 2.136468 [49664/60000]\n",
      "loss: 2.096798 [49728/60000]\n",
      "loss: 2.057160 [49792/60000]\n",
      "loss: 2.100276 [49856/60000]\n",
      "loss: 2.145005 [50176/60000]\n",
      "loss: 2.154454 [50240/60000]\n",
      "loss: 2.139696 [50304/60000]\n",
      "loss: 2.063565 [50368/60000]\n",
      "loss: 2.109544 [50688/60000]\n",
      "loss: 2.129504 [50752/60000]\n",
      "loss: 2.130670 [50816/60000]\n",
      "loss: 2.135868 [50880/60000]\n",
      "loss: 2.079462 [57344/60000]\n",
      "loss: 2.064994 [57408/60000]\n",
      "loss: 2.108972 [57472/60000]\n",
      "loss: 2.087134 [57536/60000]\n",
      "loss: 2.060860 [57856/60000]\n",
      "loss: 2.035886 [57920/60000]\n",
      "loss: 2.087198 [57984/60000]\n",
      "loss: 2.107589 [58048/60000]\n",
      "loss: 2.091096 [58368/60000]\n",
      "loss: 2.127399 [58432/60000]\n",
      "loss: 2.130913 [58496/60000]\n",
      "loss: 2.098819 [58560/60000]\n",
      "loss: 2.101181 [58880/60000]\n",
      "loss: 2.084610 [58944/60000]\n",
      "loss: 2.095202 [59008/60000]\n",
      "loss: 2.078722 [59072/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.084391 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.100515 [    0/60000]\n",
      "loss: 2.053918 [   64/60000]\n",
      "loss: 2.096573 [  128/60000]\n",
      "loss: 2.110043 [  192/60000]\n",
      "loss: 2.076293 [  512/60000]\n",
      "loss: 2.106141 [  576/60000]\n",
      "loss: 2.039719 [  640/60000]\n",
      "loss: 2.028514 [  704/60000]\n",
      "loss: 2.050974 [ 1024/60000]\n",
      "loss: 2.116500 [ 1088/60000]\n",
      "loss: 2.074862 [ 1152/60000]\n",
      "loss: 2.100931 [ 1216/60000]\n",
      "loss: 2.031865 [ 1536/60000]\n",
      "loss: 2.046687 [ 1600/60000]\n",
      "loss: 2.011790 [ 1664/60000]\n",
      "loss: 2.095844 [ 1728/60000]\n",
      "loss: 2.130996 [ 8192/60000]\n",
      "loss: 2.019197 [ 8256/60000]\n",
      "loss: 2.042507 [ 8320/60000]\n",
      "loss: 2.096750 [ 8384/60000]\n",
      "loss: 2.062879 [ 8704/60000]\n",
      "loss: 2.066052 [ 8768/60000]\n",
      "loss: 2.123697 [ 8832/60000]\n",
      "loss: 2.066150 [ 8896/60000]\n",
      "loss: 2.019378 [ 9216/60000]\n",
      "loss: 2.023466 [ 9280/60000]\n",
      "loss: 2.010250 [ 9344/60000]\n",
      "loss: 2.125435 [ 9408/60000]\n",
      "loss: 2.045166 [ 9728/60000]\n",
      "loss: 2.077581 [ 9792/60000]\n",
      "loss: 2.015911 [ 9856/60000]\n",
      "loss: 2.104620 [ 9920/60000]\n",
      "loss: 2.060399 [16384/60000]\n",
      "loss: 1.980302 [16448/60000]\n",
      "loss: 1.938429 [16512/60000]\n",
      "loss: 1.996792 [16576/60000]\n",
      "loss: 2.026359 [16896/60000]\n",
      "loss: 2.077583 [16960/60000]\n",
      "loss: 2.046139 [17024/60000]\n",
      "loss: 2.053555 [17088/60000]\n",
      "loss: 2.016833 [17408/60000]\n",
      "loss: 2.019353 [17472/60000]\n",
      "loss: 1.945283 [17536/60000]\n",
      "loss: 2.102083 [17600/60000]\n",
      "loss: 2.057381 [17920/60000]\n",
      "loss: 2.028298 [17984/60000]\n",
      "loss: 2.072432 [18048/60000]\n",
      "loss: 2.107086 [18112/60000]\n",
      "loss: 1.939072 [24576/60000]\n",
      "loss: 2.009123 [24640/60000]\n",
      "loss: 2.020042 [24704/60000]\n",
      "loss: 1.997813 [24768/60000]\n",
      "loss: 2.011377 [25088/60000]\n",
      "loss: 1.973609 [25152/60000]\n",
      "loss: 1.938847 [25216/60000]\n",
      "loss: 1.887359 [25280/60000]\n",
      "loss: 1.956184 [25600/60000]\n",
      "loss: 1.930627 [25664/60000]\n",
      "loss: 2.011218 [25728/60000]\n",
      "loss: 1.981842 [25792/60000]\n",
      "loss: 2.054797 [26112/60000]\n",
      "loss: 1.992240 [26176/60000]\n",
      "loss: 2.053674 [26240/60000]\n",
      "loss: 2.076353 [26304/60000]\n",
      "loss: 2.018236 [32768/60000]\n",
      "loss: 1.970253 [32832/60000]\n",
      "loss: 1.937892 [32896/60000]\n",
      "loss: 2.044074 [32960/60000]\n",
      "loss: 1.919513 [33280/60000]\n",
      "loss: 1.912512 [33344/60000]\n",
      "loss: 1.956095 [33408/60000]\n",
      "loss: 1.946071 [33472/60000]\n",
      "loss: 1.941980 [33792/60000]\n",
      "loss: 1.953929 [33856/60000]\n",
      "loss: 2.048321 [33920/60000]\n",
      "loss: 1.944145 [33984/60000]\n",
      "loss: 1.925141 [34304/60000]\n",
      "loss: 1.982730 [34368/60000]\n",
      "loss: 1.967545 [34432/60000]\n",
      "loss: 1.894117 [34496/60000]\n",
      "loss: 1.989215 [40960/60000]\n",
      "loss: 2.007869 [41024/60000]\n",
      "loss: 1.863918 [41088/60000]\n",
      "loss: 1.824334 [41152/60000]\n",
      "loss: 1.976076 [41472/60000]\n",
      "loss: 1.947418 [41536/60000]\n",
      "loss: 1.935184 [41600/60000]\n",
      "loss: 1.957875 [41664/60000]\n",
      "loss: 1.909183 [41984/60000]\n",
      "loss: 1.934545 [42048/60000]\n",
      "loss: 1.875098 [42112/60000]\n",
      "loss: 1.965940 [42176/60000]\n",
      "loss: 1.935364 [42496/60000]\n",
      "loss: 1.820145 [42560/60000]\n",
      "loss: 1.880460 [42624/60000]\n",
      "loss: 2.013028 [42688/60000]\n",
      "loss: 1.933374 [49152/60000]\n",
      "loss: 1.890138 [49216/60000]\n",
      "loss: 1.983945 [49280/60000]\n",
      "loss: 1.801828 [49344/60000]\n",
      "loss: 1.919881 [49664/60000]\n",
      "loss: 1.876822 [49728/60000]\n",
      "loss: 1.798802 [49792/60000]\n",
      "loss: 1.875392 [49856/60000]\n",
      "loss: 1.959511 [50176/60000]\n",
      "loss: 1.986360 [50240/60000]\n",
      "loss: 1.976487 [50304/60000]\n",
      "loss: 1.800822 [50368/60000]\n",
      "loss: 1.886114 [50688/60000]\n",
      "loss: 1.945800 [50752/60000]\n",
      "loss: 1.926420 [50816/60000]\n",
      "loss: 1.955874 [50880/60000]\n",
      "loss: 1.849110 [57344/60000]\n",
      "loss: 1.804547 [57408/60000]\n",
      "loss: 1.893886 [57472/60000]\n",
      "loss: 1.872110 [57536/60000]\n",
      "loss: 1.815178 [57856/60000]\n",
      "loss: 1.749750 [57920/60000]\n",
      "loss: 1.873130 [57984/60000]\n",
      "loss: 1.925557 [58048/60000]\n",
      "loss: 1.870832 [58368/60000]\n",
      "loss: 1.971628 [58432/60000]\n",
      "loss: 1.939683 [58496/60000]\n",
      "loss: 1.880222 [58560/60000]\n",
      "loss: 1.896154 [58880/60000]\n",
      "loss: 1.845423 [58944/60000]\n",
      "loss: 1.880516 [59008/60000]\n",
      "loss: 1.834123 [59072/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.865868 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.907389 [    0/60000]\n",
      "loss: 1.789085 [   64/60000]\n",
      "loss: 1.895399 [  128/60000]\n",
      "loss: 1.921025 [  192/60000]\n",
      "loss: 1.865079 [  512/60000]\n",
      "loss: 1.920631 [  576/60000]\n",
      "loss: 1.758336 [  640/60000]\n",
      "loss: 1.753621 [  704/60000]\n",
      "loss: 1.790199 [ 1024/60000]\n",
      "loss: 1.929158 [ 1088/60000]\n",
      "loss: 1.861745 [ 1152/60000]\n",
      "loss: 1.923190 [ 1216/60000]\n",
      "loss: 1.761407 [ 1536/60000]\n",
      "loss: 1.776980 [ 1600/60000]\n",
      "loss: 1.732191 [ 1664/60000]\n",
      "loss: 1.898841 [ 1728/60000]\n",
      "loss: 1.982477 [ 8192/60000]\n",
      "loss: 1.761002 [ 8256/60000]\n",
      "loss: 1.807957 [ 8320/60000]\n",
      "loss: 1.906888 [ 8384/60000]\n",
      "loss: 1.845528 [ 8704/60000]\n",
      "loss: 1.854429 [ 8768/60000]\n",
      "loss: 1.961591 [ 8832/60000]\n",
      "loss: 1.872352 [ 8896/60000]\n",
      "loss: 1.776272 [ 9216/60000]\n",
      "loss: 1.768220 [ 9280/60000]\n",
      "loss: 1.768029 [ 9344/60000]\n",
      "loss: 1.960963 [ 9408/60000]\n",
      "loss: 1.815164 [ 9728/60000]\n",
      "loss: 1.877487 [ 9792/60000]\n",
      "loss: 1.758578 [ 9856/60000]\n",
      "loss: 1.934468 [ 9920/60000]\n",
      "loss: 1.871923 [16384/60000]\n",
      "loss: 1.710550 [16448/60000]\n",
      "loss: 1.651231 [16512/60000]\n",
      "loss: 1.751745 [16576/60000]\n",
      "loss: 1.781007 [16896/60000]\n",
      "loss: 1.890046 [16960/60000]\n",
      "loss: 1.825670 [17024/60000]\n",
      "loss: 1.859293 [17088/60000]\n",
      "loss: 1.798331 [17408/60000]\n",
      "loss: 1.796881 [17472/60000]\n",
      "loss: 1.635656 [17536/60000]\n",
      "loss: 1.939809 [17600/60000]\n",
      "loss: 1.859917 [17920/60000]\n",
      "loss: 1.817851 [17984/60000]\n",
      "loss: 1.897006 [18048/60000]\n",
      "loss: 1.949208 [18112/60000]\n",
      "loss: 1.666911 [24576/60000]\n",
      "loss: 1.775122 [24640/60000]\n",
      "loss: 1.806443 [24704/60000]\n",
      "loss: 1.771400 [24768/60000]\n",
      "loss: 1.773240 [25088/60000]\n",
      "loss: 1.705044 [25152/60000]\n",
      "loss: 1.656364 [25216/60000]\n",
      "loss: 1.574725 [25280/60000]\n",
      "loss: 1.701107 [25600/60000]\n",
      "loss: 1.657805 [25664/60000]\n",
      "loss: 1.785818 [25728/60000]\n",
      "loss: 1.745934 [25792/60000]\n",
      "loss: 1.869726 [26112/60000]\n",
      "loss: 1.765753 [26176/60000]\n",
      "loss: 1.858868 [26240/60000]\n",
      "loss: 1.909568 [26304/60000]\n",
      "loss: 1.825293 [32768/60000]\n",
      "loss: 1.749839 [32832/60000]\n",
      "loss: 1.680663 [32896/60000]\n",
      "loss: 1.887325 [32960/60000]\n",
      "loss: 1.642771 [33280/60000]\n",
      "loss: 1.663983 [33344/60000]\n",
      "loss: 1.700719 [33408/60000]\n",
      "loss: 1.720963 [33472/60000]\n",
      "loss: 1.684557 [33792/60000]\n",
      "loss: 1.721909 [33856/60000]\n",
      "loss: 1.870642 [33920/60000]\n",
      "loss: 1.694397 [33984/60000]\n",
      "loss: 1.694344 [34304/60000]\n",
      "loss: 1.766815 [34368/60000]\n",
      "loss: 1.715619 [34432/60000]\n",
      "loss: 1.611246 [34496/60000]\n",
      "loss: 1.783507 [40960/60000]\n",
      "loss: 1.838194 [41024/60000]\n",
      "loss: 1.601372 [41088/60000]\n",
      "loss: 1.541533 [41152/60000]\n",
      "loss: 1.780464 [41472/60000]\n",
      "loss: 1.730261 [41536/60000]\n",
      "loss: 1.716504 [41600/60000]\n",
      "loss: 1.761479 [41664/60000]\n",
      "loss: 1.691215 [41984/60000]\n",
      "loss: 1.740055 [42048/60000]\n",
      "loss: 1.601560 [42112/60000]\n",
      "loss: 1.764717 [42176/60000]\n",
      "loss: 1.731054 [42496/60000]\n",
      "loss: 1.532728 [42560/60000]\n",
      "loss: 1.625087 [42624/60000]\n",
      "loss: 1.855787 [42688/60000]\n",
      "loss: 1.716011 [49152/60000]\n",
      "loss: 1.669174 [49216/60000]\n",
      "loss: 1.813294 [49280/60000]\n",
      "loss: 1.547592 [49344/60000]\n",
      "loss: 1.673459 [49664/60000]\n",
      "loss: 1.650043 [49728/60000]\n",
      "loss: 1.550265 [49792/60000]\n",
      "loss: 1.653635 [49856/60000]\n",
      "loss: 1.768016 [50176/60000]\n",
      "loss: 1.816588 [50240/60000]\n",
      "loss: 1.801387 [50304/60000]\n",
      "loss: 1.530424 [50368/60000]\n",
      "loss: 1.652893 [50688/60000]\n",
      "loss: 1.770105 [50752/60000]\n",
      "loss: 1.736173 [50816/60000]\n",
      "loss: 1.784044 [50880/60000]\n",
      "loss: 1.632680 [57344/60000]\n",
      "loss: 1.539121 [57408/60000]\n",
      "loss: 1.682511 [57472/60000]\n",
      "loss: 1.670618 [57536/60000]\n",
      "loss: 1.576438 [57856/60000]\n",
      "loss: 1.477682 [57920/60000]\n",
      "loss: 1.665486 [57984/60000]\n",
      "loss: 1.765813 [58048/60000]\n",
      "loss: 1.666098 [58368/60000]\n",
      "loss: 1.830834 [58432/60000]\n",
      "loss: 1.736724 [58496/60000]\n",
      "loss: 1.668847 [58560/60000]\n",
      "loss: 1.704815 [58880/60000]\n",
      "loss: 1.616592 [58944/60000]\n",
      "loss: 1.683318 [59008/60000]\n",
      "loss: 1.592403 [59072/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.665561 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.728859 [    0/60000]\n",
      "loss: 1.556525 [   64/60000]\n",
      "loss: 1.714165 [  128/60000]\n",
      "loss: 1.730116 [  192/60000]\n",
      "loss: 1.699446 [  512/60000]\n",
      "loss: 1.764993 [  576/60000]\n",
      "loss: 1.493191 [  640/60000]\n",
      "loss: 1.497508 [  704/60000]\n",
      "loss: 1.553320 [ 1024/60000]\n",
      "loss: 1.768195 [ 1088/60000]\n",
      "loss: 1.672794 [ 1152/60000]\n",
      "loss: 1.762930 [ 1216/60000]\n",
      "loss: 1.526449 [ 1536/60000]\n",
      "loss: 1.527863 [ 1600/60000]\n",
      "loss: 1.500882 [ 1664/60000]\n",
      "loss: 1.722749 [ 1728/60000]\n",
      "loss: 1.847194 [ 8192/60000]\n",
      "loss: 1.545109 [ 8256/60000]\n",
      "loss: 1.606164 [ 8320/60000]\n",
      "loss: 1.746467 [ 8384/60000]\n",
      "loss: 1.668240 [ 8704/60000]\n",
      "loss: 1.675446 [ 8768/60000]\n",
      "loss: 1.815296 [ 8832/60000]\n",
      "loss: 1.722562 [ 8896/60000]\n",
      "loss: 1.582006 [ 9216/60000]\n",
      "loss: 1.545557 [ 9280/60000]\n",
      "loss: 1.568408 [ 9344/60000]\n",
      "loss: 1.816757 [ 9408/60000]\n",
      "loss: 1.620531 [ 9728/60000]\n",
      "loss: 1.704286 [ 9792/60000]\n",
      "loss: 1.532862 [ 9856/60000]\n",
      "loss: 1.795672 [ 9920/60000]\n",
      "loss: 1.715223 [16384/60000]\n",
      "loss: 1.491821 [16448/60000]\n",
      "loss: 1.442324 [16512/60000]\n",
      "loss: 1.573697 [16576/60000]\n",
      "loss: 1.578607 [16896/60000]\n",
      "loss: 1.734785 [16960/60000]\n",
      "loss: 1.633498 [17024/60000]\n",
      "loss: 1.699351 [17088/60000]\n",
      "loss: 1.628284 [17408/60000]\n",
      "loss: 1.633008 [17472/60000]\n",
      "loss: 1.369697 [17536/60000]\n",
      "loss: 1.817666 [17600/60000]\n",
      "loss: 1.702976 [17920/60000]\n",
      "loss: 1.649266 [17984/60000]\n",
      "loss: 1.755805 [18048/60000]\n",
      "loss: 1.833237 [18112/60000]\n",
      "loss: 1.477548 [24576/60000]\n",
      "loss: 1.586218 [24640/60000]\n",
      "loss: 1.644363 [24704/60000]\n",
      "loss: 1.603675 [24768/60000]\n",
      "loss: 1.574501 [25088/60000]\n",
      "loss: 1.487962 [25152/60000]\n",
      "loss: 1.440694 [25216/60000]\n",
      "loss: 1.349289 [25280/60000]\n",
      "loss: 1.517238 [25600/60000]\n",
      "loss: 1.450761 [25664/60000]\n",
      "loss: 1.617490 [25728/60000]\n",
      "loss: 1.558756 [25792/60000]\n",
      "loss: 1.729145 [26112/60000]\n",
      "loss: 1.610160 [26176/60000]\n",
      "loss: 1.716092 [26240/60000]\n",
      "loss: 1.788421 [26304/60000]\n",
      "loss: 1.675370 [32768/60000]\n",
      "loss: 1.598175 [32832/60000]\n",
      "loss: 1.504601 [32896/60000]\n",
      "loss: 1.790293 [32960/60000]\n",
      "loss: 1.438735 [33280/60000]\n",
      "loss: 1.505925 [33344/60000]\n",
      "loss: 1.503845 [33408/60000]\n",
      "loss: 1.574895 [33472/60000]\n",
      "loss: 1.493681 [33792/60000]\n",
      "loss: 1.569104 [33856/60000]\n",
      "loss: 1.739093 [33920/60000]\n",
      "loss: 1.515351 [33984/60000]\n",
      "loss: 1.548193 [34304/60000]\n",
      "loss: 1.614414 [34368/60000]\n",
      "loss: 1.530458 [34432/60000]\n",
      "loss: 1.416567 [34496/60000]\n",
      "loss: 1.632453 [40960/60000]\n",
      "loss: 1.727324 [41024/60000]\n",
      "loss: 1.433184 [41088/60000]\n",
      "loss: 1.354780 [41152/60000]\n",
      "loss: 1.647062 [41472/60000]\n",
      "loss: 1.573211 [41536/60000]\n",
      "loss: 1.570821 [41600/60000]\n",
      "loss: 1.645915 [41664/60000]\n",
      "loss: 1.548612 [41984/60000]\n",
      "loss: 1.619662 [42048/60000]\n",
      "loss: 1.411688 [42112/60000]\n",
      "loss: 1.620097 [42176/60000]\n",
      "loss: 1.596194 [42496/60000]\n",
      "loss: 1.353256 [42560/60000]\n",
      "loss: 1.458045 [42624/60000]\n",
      "loss: 1.761324 [42688/60000]\n",
      "loss: 1.554624 [49152/60000]\n",
      "loss: 1.523902 [49216/60000]\n",
      "loss: 1.697405 [49280/60000]\n",
      "loss: 1.398402 [49344/60000]\n",
      "loss: 1.501527 [49664/60000]\n",
      "loss: 1.500598 [49728/60000]\n",
      "loss: 1.401525 [49792/60000]\n",
      "loss: 1.517840 [49856/60000]\n",
      "loss: 1.640177 [50176/60000]\n",
      "loss: 1.704944 [50240/60000]\n",
      "loss: 1.673441 [50304/60000]\n",
      "loss: 1.351152 [50368/60000]\n",
      "loss: 1.498417 [50688/60000]\n",
      "loss: 1.660668 [50752/60000]\n",
      "loss: 1.615363 [50816/60000]\n",
      "loss: 1.674241 [50880/60000]\n",
      "loss: 1.491557 [57344/60000]\n",
      "loss: 1.356661 [57408/60000]\n",
      "loss: 1.533373 [57472/60000]\n",
      "loss: 1.543754 [57536/60000]\n",
      "loss: 1.422540 [57856/60000]\n",
      "loss: 1.307147 [57920/60000]\n",
      "loss: 1.531247 [57984/60000]\n",
      "loss: 1.667829 [58048/60000]\n",
      "loss: 1.528924 [58368/60000]\n",
      "loss: 1.734403 [58432/60000]\n",
      "loss: 1.586431 [58496/60000]\n",
      "loss: 1.527798 [58560/60000]\n",
      "loss: 1.582976 [58880/60000]\n",
      "loss: 1.467705 [58944/60000]\n",
      "loss: 1.559352 [59008/60000]\n",
      "loss: 1.430048 [59072/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.536727 \n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:51.902414Z",
     "start_time": "2024-01-20T06:37:37.181165Z"
    }
   },
   "id": "73530fbaca865d1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 저장하기\n",
    "\n",
    "모델을 저장하는 일반적인 방법은 (모델의 매개변수들을 포함햐여) 내부 상태사전(internal state dictionary)을 직렬화(serialize)하는 것입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0c074b5c615b551"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:51.906334Z",
     "start_time": "2024-01-20T06:37:51.903369Z"
    }
   },
   "id": "da0f9ad635443418"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 불러오기\n",
    "모델을 불러오는 과정에서 모델 구조를 다시 만들고 상태 사전을 모델에 불러오는 과정이 포함됩니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73ac7e1659797168"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:51.916289Z",
     "start_time": "2024-01-20T06:37:51.905977Z"
    }
   },
   "id": "17db71b8e5a41030"
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델을 사용하여 예측해봅니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "521d3a7d6162f502"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:51.916960Z",
     "start_time": "2024-01-20T06:37:51.913466Z"
    }
   },
   "id": "38ae51bab773a233"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:37:51.921234Z",
     "start_time": "2024-01-20T06:37:51.916127Z"
    }
   },
   "id": "cb4c579b9d500faa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
