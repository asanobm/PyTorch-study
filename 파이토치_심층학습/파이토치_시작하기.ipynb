{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/asanobm/.cache/huggingface/token\n",
      "Login successful\n",
      "디바이스: mps\n"
     ]
    }
   ],
   "source": [
    "# INIT\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"디바이스: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치 기초\n",
    "\n",
    "## 텐서(Tensor)\n",
    "\n",
    "파이토치는 텐서(Tensor)를 사용하여 모델의 입력과 출력, 그리고 모델의 매개변수를 부호화(encode)한다. 텐서는 NumPy의 ndarray와 유사하며, GPU를 사용한 연산 가속도 가능하다.\n",
    "\n",
    "### 텐서 초기화하기\n",
    "```python\n",
    "import torch\n",
    "\n",
    "tensor = torch.rand(2, 3)\n",
    "print(tensor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "텐서의 타입: torch.FloatTensor\n",
      "텐서의 크기: torch.Size([4])\n",
      "텐서의 디바이스: cpu\n",
      "텐서의 레이아웃: torch.strided\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([1, 2, 3, 4])\n",
    "print(tensor)\n",
    "# 텐서의 속성\n",
    "print(f\"텐서의 타입: {tensor.type()}\")\n",
    "print(f\"텐서의 크기: {tensor.shape}\")\n",
    "print(f\"텐서의 디바이스: {tensor.device}\")\n",
    "print(f\"텐서의 레이아웃: {tensor.layout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.], device='mps:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change device\n",
    "tensor.cpu().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설\n",
    "\n",
    "가설(Hypothesis)이란 어떤 사실을 설명하거나 증명하기 위한 가정으로 두개 이상의 변수 관계를 검증 가능한 형태로 기술하여 변수 간의 관계를 예측하는 것을 의미한다.\n",
    "\n",
    "> page 55\n",
    "\n",
    "가설은\n",
    "연구가설(Research Hypothesis)과 대립가설(Alternative Hypothesis), 그리고 귀무가설(Null Hypothesis)로 구분된다.\n",
    "\n",
    "* 연구가설은 연구자가 연구를 통해 증명하고자 하는 가설이다. 연구가설은 연구자가 연구를 통해 증명하고자 하는 가설이다.\n",
    "* 귀무가설은 통계학에서 처음부터 버릴 것으로 예상되는 가설이다.\n",
    "* 대립가설은 귀무가설의 반대 가설이다.\n",
    "\n",
    "## 머신러닝에서 가설\n",
    "\n",
    "머신러닝에서 가설은 독립변수(X)와 종속변수(Y)의 관계를 예측하는 함수이다.\n",
    "\n",
    "* 가설은 **단일 가설(Single Hypohesis)**과 **가설 집합(Hypothesis Set)**으로 표현할 수 있다.\n",
    "* 단일 가설은 입력을 출력에 매핑하고 평가하고 예측하는데 사용한다.\n",
    "* 가설 집합은 모델이 학습하는 동안 단일 가설을 변경하고 평가하는데 사용한다.\n",
    "\n",
    "## 통계적 가설 검정\n",
    "\n",
    "대표적인 통계적 가설은 다음과 같다.\n",
    "\n",
    "* t-검정(t-test)은 두 집단의 평균이 유의미한지를 검정하는 방법이다. t-검정은 두 집단의 평균이 유의미한지를 검정하는 방법이다.\n",
    "* 쌍체t-검정(Paired t-test)은 두 집단의 평균이 유의미한지를 검정하는 방법이다.\n",
    "* 비쌍체t-검정(Unpaired t-test)은 두 집단의 평균이 유의미한지를 검정하는 방법이다.\n",
    "\n",
    "\n",
    "머신러닝의 통계적 가설을 적용한다면 비쌍체t-검정을 사용해야 한다. 독립 변수(X)와 종속 변수(Y)의 관계를 예측하는 함수를 찾기 위해 머신러닝 모델을 학습시키는데, \n",
    "이때 학습 데이터와 검증 데이터를 나누어 학습 데이터로 학습을 하고 검증 데이터로 검증을 한다. 이때 학습 데이터와 검증 데이터는 서로 다른 데이터이므로 비쌍체t-검정을 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사람의 키(cm)가 설병과 관련 있는지 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m      4\u001b[0m man_height \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mrvs(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m170\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "man_height = stats.norm.rvs(loc=170, scale=10, size=500, random_state=1)\n",
    "woman_height = stats.norm.rvs(loc=150, scale=10, size=500, random_state=1)\n",
    "\n",
    "X=np.concatenate([man_height, woman_height])\n",
    "Y=['man'] * len(man_height) + ['woman'] * len(woman_height)\n",
    "\n",
    "df = pd.DataFrame(list(zip(X,Y)), columns=[\"X\", \"Y\"])\n",
    "fig = sns.displot(data=df, x=\"X\", hue=\"Y\", kind=\"kde\")\n",
    "fig.set_axis_labels(\"cm\", \"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.norm.rvs는 특정 평균(loc)과 표준편차(scale)를 가지는 정규분포에서 무작위 표본을 추출한다.\n",
    "\n",
    "비쌍체 검정을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic: 31.96162891312776\n",
      "pvalue: 6.2285854381989205e-155\n",
      "*: True\n",
      "**: True\n"
     ]
    }
   ],
   "source": [
    "statistic, pvalue = stats.ttest_ind(man_height, woman_height, equal_var=True)\n",
    "\n",
    "print(f\"statistic: {statistic}\")    \n",
    "print(f\"pvalue: {pvalue}\")\n",
    "print(f\"*: {pvalue < 0.05}\")\n",
    "print(f\"**: {pvalue < 0.01}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성별 차이에 대한 유의미성을 판단하기 위해 통계량(statistic)또는 유의확률(p-value)을 확인, 토계량이 크고 유의확률이 작다면 귀무가설이 참일 확률이 낮다고 할 수 있다.\n",
    "\n",
    "즉 \"남녀 키의 평균이 서로 같다\"의 확률이 낮다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수\n",
    "\n",
    "손실 함수(Loss Function)는 실제값과 예측값의 차이를 계산하는 함수이다.\n",
    "실제값과 예측값을 통해 계산된 오차값을 최소화해 정확도를 높이는 방법으로 학습이 진행된다. 손실함수는 목적함수(Objective Function), 비용함수(Cost Function), 라고도 한다.\n",
    "\n",
    "* 목적함수: 함수값의 결과를 최대값 또는 최소값으로 최적화하는 함수이다.\n",
    "* 비용함수: 전체 데이터에 대한 오차를 계산하는 함수이다.\n",
    "\n",
    "$손실함수 \\subset 목적함수 \\subset 비용함수$의 포함관계를 가진다.\n",
    "\n",
    "### 제곱 오차(Squared Error)\n",
    "\n",
    "평균 제곱오차(Mean Squared Error, MSE)는 실제값과 예측값의 차이를 제곱하여 평균한 것이다. 제곱 오차는 회귀 문제를 풀 때 자주 사용되는 손실 함수이다.\n",
    "\n",
    "제곱 오차 계산 방식: $SE = (Y_{i}-\\hat{Y_{i}})^{2}$\n",
    "\n",
    "제곱 오차에서는 실제값과 예측값을 감산한 값에 제곰을 취하는데, 만약 제곱을 취하지 않으면 오차가 양의 방향인지 음의 방향인지 알 수 있다. 하지만 오차에서는 오차의 방향보다 오차의 크기에 초점을 두기 때문에 제곱을 취하여 오차가 항상 양수가 되도록 한다.\n",
    "\n",
    "제곱이 아닌 절댓값을 취해 오차의 크기를 확인할 수도 있다. 하지만 오차의 간극을 더 크게 인식하게 되므로 제곱 오차를 사용한다.\n",
    "\n",
    "### 오차 제곱합\n",
    "\n",
    "오차 제곱합(Sum of Squared Error, SSE)은 오차를 모두 더한 값을 의미한다. 제곰 오차는 각 데이터의 오차를 의미하므로 가설 또는 모델 자체가 얼마나 정확한지를 평가하는 척도로 사용된다. 오차 제곱합은 제곱 오차를 모두 더한 값이므로 제곱 오차보다 더 큰 값을 가진다.\n",
    "\n",
    "오차 제곱합 계산 방식: $SSE = \\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})^{2}$\n",
    "\n",
    "오차 제곱합에서 오찻값들을 제곱하지 안혹 모두 더하면 음수가 나오는 오류가 발생할 수 있다. 따라서 오차 제곱합에서는 오차를 제곱한 후 모두 더한다.\n",
    "\n",
    "### 평균 제곱 오차\n",
    "\n",
    "평균 제곱 오차(Mean Squared Error, MSE)는 제곱 오차를 데이터의 개수로 나눈 것이다. 평균 제곱 오차는 제곱 오차와 오차 제곱합의 크기를 줄이는 역할을 한다.\n",
    "\n",
    "평균 제곱 오차 계산 방식: $MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})^{2}$\n",
    "\n",
    "### 교차 엔트로피\n",
    "\n",
    "교차 엔트로피(Cross Entropy)는 분류 문제에서 사용되는 손실 함수이다. 교차 엔트로피는 실제값이 1일 때의 예측값의 자연로그를 계산한다. 교차 엔트로피는 실제값이 1일 때의 예측값이 1일수록 오차가 작아지고, 실제값이 0일 때의 예측값이 0일수록 오차가 작아진다.\n",
    "\n",
    "교차 엔트로피 계산 방식: $CE = -\\sum_{i=1}^{n}y_{i}log(\\hat{y_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화\n",
    "\n",
    "최적화(Optimization)란 목적 함수의 결괏값을 최적화하는 변수를 찾는 알고리즘이다. 손실 함수에서 인공 신경망은 오찻값을 최소화하여 학습 데이터에 대한 가설의 정확도를 높이는 방법으로 학습이 진행된다. 머신러닝은 손실 함수를 활용해 최적의 해법이나 변수를 찾는 것이다.\n",
    "\n",
    "* 손실 함수의 값이 최소가 되는 변수를 찾는다면 새로운 데이터에 대해 더 정교한 예측을 할 수 있다.\n",
    "* 최적화 알고리즘은 실젯값과 예측값의 차이를 계산해 오차를 최소로 줄일 수 있는 가중치와 편향을 계산한다.\n",
    "\n",
    "최적의 가중치와 편향을 갖는 가설은 오찻값이 0에 가까운 함수가 된다.(도함수의 변화량이 0에 가깝다는 뜻)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사 하강법\n",
    "\n",
    "경사 하강법(Gradient Descent)은 최적화 알고리즘 중 하나이다. 경사 하강법은 함수의 기울기가 낮은 곳으로 계속 이동시켜 극값에 도달할 때 까지 반복시키는 방법이다.\n",
    "\n",
    "$$\\displaystyle \\mathbf {x} _{i+1}=\\mathbf {x} _{i}-\\gamma _{i}\\nabla f(\\mathbf {x} _{i})$$\n",
    "\n",
    "경사 하강법을 포함한 최적화 함수는 초기값$\\mathbf {x} _{0}$을 설정하고, 초기값에서의 기울기$\\nabla f(\\mathbf {x} _{0})$를 계산한다. 그리고 기울기가 낮은 방향으로 이동시키는데, 이때 이동하는 거리를 학습률(learning rate)이라고 한다. 학습률은 0과 1 사이의 값으로 설정하며, 학습률이 너무 작으면 최소값에 도달하는 시간이 오래 걸리고, 학습률이 너무 크면 최소값을 찾지 못하고 발산한다.\n",
    "\n",
    "### 가중치 갱신 방법\n",
    "가설은$\\hat{Y} = W_{i}*x+b_{i}$로 하고, 손실 함수는 평균 제곱 오차를 적용\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{i} = W_{i}x+b_{i} \\\\\n",
    "MSE(W,b) = {1 \\over n}\\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})^{2}\n",
    "$$\n",
    "\n",
    "가중치의 기울기를 확인하기 위해W에 대해 편미분한다.\n",
    "\n",
    "$$\n",
    "W_{i+1} = W_{i} - \\alpha \\frac{\\partial}{\\partial W} \\frac{1}{n} \\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})^{2} \\\\\n",
    "        = W_{i} - \\alpha \\frac{\\partial}{\\partial W} \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{1}{n}\\{Y_{i}-(W_{i}x_b)\\}^2] \\\\\n",
    "        = W_{i} - \\alpha \\frac{2}{n} \\sum_{i=1}^{n}[Y_{i}-(W_{i}x+b_{i})(-x)] \\\\\n",
    "        = W_{i} - \\alpha \\frac{2}{n} \\sum(Y_{i}-\\hat{Y_{i}})(-x) \\\\\n",
    "        = W_{i} + \\alpha \\frac{2}{n} \\sum(Y_{i}-\\hat{Y_{i}})x \\\\\n",
    "        = W_{i} + \\alpha 2E[(\\hat{Y_{i}}-Y_{i})x]\n",
    "$$\n",
    "\n",
    "일반 계산\n",
    "$$\n",
    "W_{i+1} = W_{i} - \\alpha E[(\\hat{Y_{i}}-Y_{i})x]\n",
    "$$\n",
    "\n",
    "경사 하강법을 적용한 새로운 가중치 수식은$W_{i+1} = W_{i} - \\alpha E[(\\hat{Y_{i}}-Y_{i})x]$이다. 이때 가중치의 기울기는$E[(\\hat{Y_{i}}-Y_{i})x]$이다. 가중치의 기울기는 가중치가 증가할 때 손실 함수가 증가하는지 감소하는지를 나타낸다. 가중치의 기울기가 양수이면 가중치가 증가할 때 손실 함수가 증가하므로 가중치를 감소시켜야 한다. 가중치의 기울기가 음수이면 가중치가 증가할 때 손실 함수가 감소하므로 가중치를 증가시켜야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습률\n",
    "\n",
    "머신러닝에서 $\\alpha$는 학습률(learning rate)이다. 초깃값($W_{0}$)을 임의의 값으로 설정하듯 합습률($\\alpha$)도 임의의 값으로 설정한다. 학습률에 따라 다음 가중치($W_{1}W_{2}W_{3}...$)가 결정된다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 문제\n",
    "\n",
    "최솟값, 극솟값이 존재하는 데이터에서 기울기가 0이되는 극값은 최댓값, 최솟값, 극댓값, 극솟값으로 구분할 수 있다. 시작점 또는 학습률이 낮으면 최솟값을 찾지 못하고 극솟값에 도달할 수 있다. 또한 안장점(saddle point)이라는 기울기가 0이 되는 지점이 존재한다. 안장점은 기울기가 0이지만 극댓값도 극솟값도 아닌 지점이다.\n",
    "\n",
    "최적화 알고리즘은 경사 하강법처럼 목적 함수가 최적의 값을 찾아갈 수 있도록 도와주는 알고리즘이다. 최적화 알고리즘은 경사 하강법 외에도 다양한 알고리즘이 존재한다.\n",
    "\n",
    "* 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
    "* 모멘텀(Momentum)\n",
    "* 아다그라드(Adagrad)\n",
    "* 알엠에스프롭(RMSprop)\n",
    "* 아담(Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단순 선형 회귀\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: :1000, w: 0.872, b: -0.277, loss: 1.362\n",
      "Epoch: :2000, w: 0.876, b: -0.374, loss: 1.358\n",
      "Epoch: :3000, w: 0.878, b: -0.404, loss: 1.358\n",
      "Epoch: :4000, w: 0.878, b: -0.413, loss: 1.358\n",
      "Epoch: :5000, w: 0.879, b: -0.415, loss: 1.358\n",
      "Epoch: :6000, w: 0.879, b: -0.416, loss: 1.358\n",
      "Epoch: :7000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :8000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :9000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :10000, w: 0.879, b: -0.417, loss: 1.358\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([\n",
    "    [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n",
    "    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n",
    "    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30],\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1],\n",
    "    [8.46], [9.5], [10.67], [11.6], [14], [11.83], [14.4], [14.25], [16.2], [16.32],\n",
    "    [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8],\n",
    "])\n",
    "\n",
    "# 초기화\n",
    "w = 0.0 # 가중치\n",
    "b = 0.0 # 편향\n",
    "lr=0.005 # 학습률\n",
    "\n",
    "def forward(w, b, lr):\n",
    "    for epoch in range(10000):\n",
    "        y_pred = w * x + b\n",
    "        loss = ((y - y_pred) ** 2).mean()\n",
    "        \n",
    "        w = w - lr * ((y_pred - y) * x).mean()\n",
    "        b = b - lr * (y_pred - y).mean()\n",
    "        \n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f\"Epoch: :{epoch+1:4d}, w: {w:.3f}, b: {b:.3f}, loss: {loss:.3f}\")\n",
    "\n",
    "forward(w, b, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: :1000, w: 0.861, b: -0.055, loss: 1.389\n",
      "Epoch: :2000, w: 0.865, b: -0.131, loss: 1.377\n",
      "Epoch: :3000, w: 0.868, b: -0.191, loss: 1.370\n",
      "Epoch: :4000, w: 0.870, b: -0.239, loss: 1.365\n",
      "Epoch: :5000, w: 0.872, b: -0.277, loss: 1.362\n",
      "Epoch: :6000, w: 0.873, b: -0.306, loss: 1.361\n",
      "Epoch: :7000, w: 0.874, b: -0.329, loss: 1.359\n",
      "Epoch: :8000, w: 0.875, b: -0.348, loss: 1.359\n",
      "Epoch: :9000, w: 0.876, b: -0.362, loss: 1.358\n",
      "Epoch: :10000, w: 0.876, b: -0.374, loss: 1.358\n"
     ]
    }
   ],
   "source": [
    "w = 0.0\n",
    "b = 0.0\n",
    "lr=0.001\n",
    "forward(w, b, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: :1000, w: 0.873, b: -0.306, loss: 1.361\n",
      "Epoch: :2000, w: 0.877, b: -0.390, loss: 1.358\n",
      "Epoch: :3000, w: 0.878, b: -0.410, loss: 1.358\n",
      "Epoch: :4000, w: 0.879, b: -0.415, loss: 1.358\n",
      "Epoch: :5000, w: 0.879, b: -0.416, loss: 1.358\n",
      "Epoch: :6000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :7000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :8000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :9000, w: 0.879, b: -0.417, loss: 1.358\n",
      "Epoch: :10000, w: 0.879, b: -0.417, loss: 1.358\n"
     ]
    }
   ],
   "source": [
    "w = 0.0\n",
    "b = 0.0\n",
    "lr=0.006\n",
    "forward(w, b, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단순 선형 회귀: 파이토치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 1]), torch.Size([30, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor(x)\n",
    "y = torch.FloatTensor(y)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: :1000, w: 0.865, b: -0.131, loss: 1.377\n",
      "Epoch: :2000, w: 0.870, b: -0.239, loss: 1.365\n",
      "Epoch: :3000, w: 0.873, b: -0.306, loss: 1.361\n",
      "Epoch: :4000, w: 0.875, b: -0.348, loss: 1.359\n",
      "Epoch: :5000, w: 0.876, b: -0.374, loss: 1.358\n",
      "Epoch: :6000, w: 0.877, b: -0.390, loss: 1.358\n",
      "Epoch: :7000, w: 0.878, b: -0.400, loss: 1.358\n",
      "Epoch: :8000, w: 0.878, b: -0.406, loss: 1.358\n",
      "Epoch: :9000, w: 0.878, b: -0.410, loss: 1.358\n",
      "Epoch: :10000, w: 0.878, b: -0.413, loss: 1.358\n"
     ]
    }
   ],
   "source": [
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "lr=0.001\n",
    "\n",
    "optimizer = optim.SGD([w, b], lr=lr)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y - y_pred) ** 2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch: :{epoch+1:4d}, w: {w.item():.3f}, b: {b.item():.3f}, loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치와 편향이 넘파이 방식과 동일하다(?) 학습률을 0.001로 기존 경사 하강법보다 낮은 학습률을 선택했지만, 확률적 경사 하강법을 활용해 더 빠른 속도로 최적의 가중치와 편향을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**zero_grad()**: 기울기를 0으로 초기화한다. 기울기가 누적되지 않도록 반복할 때마다 기울기를 0으로 초기화한다.\n",
    "**backward()**: 기울기를 계산한다. 기울기를 계산한 후에는 가중치와 편향을 갱신한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
