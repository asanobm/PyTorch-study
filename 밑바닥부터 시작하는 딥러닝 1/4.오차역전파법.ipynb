{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오차역전파법\n",
    "\n",
    "* 신경망 학습에서 가중치 가중치 매개변수에 대한 손실함수의 기울기를 수치 미분을 사용했다.\n",
    "* 수치 미분은 단순하고 구현하기 쉽지만 계산 시간이 오래 걸린다.\n",
    "* 가중치 매개변수의 기울기를 효율적으로 계산하는 방법이 오차역전파법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. 계산 그래프\n",
    "\n",
    "**계산 그래프**는 계산과정을 그래프로 나타낸 것. 복수의 노드와 에지로 표현된다.\n",
    "\n",
    "### 5.1.1. 계산 그래프로 풀다\n",
    "\n",
    "문제: 사과 2개를 샀을 때의 지불 금액을 구하라.\n",
    "현빈이는 사과 2개를 샀고, 사과 한 개에 100원이다. 소비세가 10% 부과된다고 할 때 지불 금액을 구하라.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "  A[사과] -->|100| B((x2)) -->|200| C((x1.1)) -->|220| D[지불금액]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수로 취급해서 계산 그래프로 나타내면 다음과 같다.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "  A[사과]\n",
    "  C[사과의 개수] \n",
    "  T[소비세]\n",
    "\n",
    "  x1((x))\n",
    "  x2((x))\n",
    "  r[끝]\n",
    "\n",
    "  A -->|100| x1\n",
    "  C -->|2| x1\n",
    "  T -->|1.1| x2\n",
    "  x1 -->|200| x2\n",
    "  x2 -->|220| r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문제2**\n",
    "* 현빈 군은 슈퍼에서 사과를 2개, 귤을 3개 샀다.\n",
    "* 사과는 1개에 100원, 귤은 1개에 150원이다.\n",
    "* 소비세는 10%이다. 지불 금액을 구하라.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "  A[사과]\n",
    "  B[귤]\n",
    "  C[사과의 개수]\n",
    "  D[귤의 개수]\n",
    "  T[소비세]\n",
    "  R[지불금액]\n",
    "\n",
    "  x1((x))\n",
    "  x2((x))\n",
    "  x3((x))\n",
    "\n",
    "  s1((+))\n",
    "\n",
    "  A -->|100| x1\n",
    "  C -->|2| x1\n",
    "\n",
    "  B -->|150| x2\n",
    "  D -->|3| x2\n",
    "\n",
    "  x1 -->|200| s1\n",
    "  x2 -->|450| s1\n",
    "\n",
    "  s1 -->|650| x3\n",
    "  T -->|1.1| x3\n",
    "  x3 -->|715| R\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**순전파 역전파**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "  x[x]\n",
    "  y[y]\n",
    "  f((f))\n",
    "\n",
    "x -->|x| f\n",
    "f -->|y| y\n",
    "y -->|E| f\n",
    "f -->|E*ay/ax|x\n",
    "```\n",
    "\n",
    "계산그래프에서 계산을 왼쪽에서 오른쪽으로 진행하는 단계를 **순전파**라고 한다.\n",
    "\n",
    "그 반대로 오른쪽에서 왼쪽으로 진행하는 단계를 **역전파**라고 한다.\n",
    "\n",
    "**역전파 계산 절차**\n",
    "* 신호E에 노드의 국소적 미분$\\frac{\\partial f}{\\partial x}$을 곱한 값을 다음 노드로 전달한다.\n",
    "* 국소적 미분은 순전파 때의 $y=f(x)$ 계산의 미분을 구한다는 거다, 이는 $x$에 대한 $y$의 미분$\\frac{\\partial y}{\\partial x}$을 구하는 것이다.\n",
    "* $y=f(x)=x^2$의 미분은 $\\frac{\\partial y}{\\partial x}=2x$이다, 따라서 $E\\frac{\\partial f}{\\partial x}$를 상류에 전달하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. 국소적 계산\n",
    "\n",
    "계산 그래프의 특징은 국소적 계산을 전파함으로써 최종 결과를 얻는다는 것이다.\n",
    "\n",
    "국소적이란 자신과 직접 관계된 작은 범위라는 뜻이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. 왜 계산 그래프로 푸는가?\n",
    "\n",
    "**계산 그래프의 이점**\n",
    "\n",
    "* 전체 계산이 아무리 복잡하더라도 각 노드에서의 계산은 단순한 계산으로 이루어진다.\n",
    "* 중간 계산 결과를 모두 보관할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. 연쇄법칙\n",
    "\n",
    "역전파는 국소적인 미분을 순방향과는 반대 방향으로 전파하는 것이다. 이때 '국소적 미분'을 전달하는 원리가 **연쇄법칙**이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. 계산 그래프의 역전파\n",
    "\n",
    "역전파의 계산 절차는 다음과 같다.\n",
    "\n",
    "$y=f(x)$라는 계산의 역전파는 $y$에 대한 $x$의 미분을 구하는 것이다.\n",
    "\n",
    "신호 E에 노드의 국소적 미분($\\frac{\\partial y}{\\partial x}$)을 곱한 후 다음 노드로 전달한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. 연쇄법칙이란?\n",
    "\n",
    "*합성 함수*란 여러 함수로 구성된 함수를 말한다. $z = (x + y)^2$라는 식은 $z = t^2$와 $t = x + y$라는 두 개의 식으로 구성된다.\n",
    "\n",
    "> 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}\n",
    "$$\n",
    "\n",
    "다음은 미분 $\\frac{\\partial z}{\\partial t}$의 국소적 미분(편미분)을 구한다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial t} = 2t\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial t}{\\partial x} = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial z}{\\partial t}$는 $2t$이고, $\\frac{\\partial t}{\\partial x}$는 1이므로\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t \\cdot 1 = 2(x + y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3. 연쇄법칙과 계산 그래프\n",
    "\n",
    "계산 그래프의 역전파는 연쇄법칙에 따라 각 노드에서의 미분을 곱한다.\n",
    "\n",
    "2제곱 계산을 `**2`로 표시하고, 덧셈 계산을 `+`로 표시하면 다음과 같다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. 역전파\n",
    "\n",
    "계산 그래프의 역전파가 연쇄법칙에 따라 진행된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. 덧셈 노드의 역전파\n",
    "\n",
    "덧셈 노드의 연전파법 $z=x+y$라고 하면, 미분은 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = 1\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "식에서 $\\frac{\\partial z}{\\partial x}$와 $\\frac{\\partial z}{\\partial y}$는 모두 1이 된다.\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 덧셈 노드 계산 그래프\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dot1 \u001b[38;5;241m=\u001b[39m graphviz\u001b[38;5;241m.\u001b[39mDigraph(comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Round Table\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LR -> Left to Right (default)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda uninstall -y graphviz\n",
    "!conda uninstall -y python-graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. 곱셈 노드의 역전파\n",
    "\n",
    "$z=xy$라고 하면, 미분은 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보낸다.\n",
    "\n",
    "서로 바꾼 값이란 순전파 때 x였다면 역전파에서는 y가 되고, y였다면 x가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. 사과 쇼핑의 예\n",
    "\n",
    "곱셈 노드의 역전파에서는 입력신호를 서로 바꿔서 하류로 보낸다.\n",
    "\n",
    "사과 가격의 미분은 2.2, 사과 개수의 미분은 110이다. 소비세 미분은 200이다.\n",
    "\n",
    "이는 사과 가격이 같은 양만큼 오르면 최종 금액에는 소비세가 200의 크기로, 사과 가격이 2.2의 크기로 반영된다는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. 단순한 계층 구현하기\n",
    "\n",
    "사과 쇼핑의 계산 그래프를 파이썬으로 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1. 곱셈 계층\n",
    "\n",
    "모든 계층은 forward()와 backward()라는 메서드를 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "  def __init__(self):\n",
    "    self.x = None\n",
    "    self.y = None\n",
    "    \n",
    "  def forward(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    out = x * y\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * self.y # xとyをひっくり返す\n",
    "    dy = dout * self.x # xとyをひっくり返す\n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__()`에서는 인스턴스 변수인 x와 y를 초기화한다.\n",
    "\n",
    "이 두 변수는 순전파 시의 입력값을 유지하기 위해 사용한다.\n",
    "\n",
    "`forward()`에서는 입력받은 x와 y를 곱해서 반환한다.\n",
    "\n",
    "`backward()`에서는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 서로 바꿔 곱한 후 하류로 흘린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(int(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 변수에 대한 미분은 `backward()`에서 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파\n",
    "\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward()`호출 순서는 `forward()`때와는 반대이다.\n",
    "\n",
    "`backward()`가 받은 인수는 `순전파의 출력에 대한 미분`임에 주의하자.\n",
    "\n",
    "mul_apple_layer라는 곱셈 계층은 순전파 때는 apple_price를 추력하지만, 역전파 때는 apple_price의 미분 값인 dapple_price를 인수로 받는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2. 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def forward(self, x, y):\n",
    "    out = x + y\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * 1\n",
    "    dy = dout * 1\n",
    "    return dx, dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덧셈 계층은 초기화가 필요없다.\n",
    "\n",
    "덧셈 계층의`forward()`에서는 입력 받은 두 인수 x, y를 더해서 반환한다.\n",
    "\n",
    "`backward()`에서는 상류에서 넘어온 미분(dout)을 그대로 하류로 흘린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(int(dapple_num), dapple, int(dorange), dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. 활성화 함수 계층 구현하기\n",
    "\n",
    "### 5.5.1. ReLU 계층\n",
    "\n",
    "ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.\n",
    "$$\n",
    "y = \n",
    "  \\begin{cases} x & (x > 0) \\\\\n",
    "  0 & (x \\leq 0) \\end{cases}\n",
    "$$\n",
    "\n",
    "미분은 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \n",
    "  \\begin{cases} 1 & (x > 0) \\\\\n",
    "  0 & (x \\leq 0) \\end{cases}\n",
    "$$\n",
    "\n",
    "순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다.\n",
    "\n",
    "반면, 순전파 때 x가 0 이하면 역전파 때는 하류로 신호를 보내지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "  def __init__(self):\n",
    "    self.mask = None\n",
    "    \n",
    "  def forward(self, x):\n",
    "    self.mask = (x <= 0)\n",
    "    out = x.copy()\n",
    "    out[self.mask] = 0\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dout[self.mask] = 0\n",
    "    dx = dout\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu 클래스는 mask라는 변수를 가지며, mask는 True/False로 구성된 넘파이 배열로, 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외(0보다 큰 원소)는 False로 유지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "\n",
    "print(x)\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2. Sigmoid 계층\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + \\exp(-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1단계**\n",
    "\n",
    "'/'노드는 $y = \\frac{1}{x}$로 표현할 수 있다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2}\n",
    "$$\n",
    "$$\n",
    "=-y^2\n",
    "$$\n",
    "\n",
    "식에 따르면 역전파 때는 상류에서 흘러온 값에 $-y^2$을 곱해서 하류로 전달한다.\n",
    "\n",
    "**2단계**\n",
    "'+'노드는 상류의 값을 그대로 하류로 흘린다.\n",
    "\n",
    "**3단계**\n",
    "'exp'노드는 $y = \\exp(x)$로 표현할 수 있다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\exp(x)\n",
    "$$\n",
    "\n",
    "**4단계**\n",
    "'x'노드는 순전파 때의 값을 서로 바꿔 곱한다.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y}y^2exp(-x)$는 다음과 같이 표현할 수 있다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y}y^2exp(-x) = \\frac{\\partial L}{\\partial y}y(1-y)\n",
    "$$\n",
    "\n",
    "시그모이드 계층의 역전파는 위와 같이 구할 수 있다.\n",
    "\n",
    "파이썬으로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "  def __init__(self):\n",
    "    self.out = None\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    self.out = out\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * (1.0 - self.out) * self.out\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Affine/Softmax 계층 구현하기\n",
    "\n",
    "### 5.6.1. Affine 계층\n",
    "\n",
    "신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서는 어파인 변환(Affine Transformation)이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(2) # 입력\n",
    "W = np.random.rand(2, 3) # 가중치\n",
    "B = np.random.rand(3) # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 X, W, B는 각각 입력, 가중치, 편향을 뜻한다. 형상은 (2,) (2,3) (3,)이다.\n",
    "\n",
    "뉴런의 가중치 합은 $Y=XW+B$로 나타낼 수 있다. 그리고 이 Y를 활성화 함수로 변환해 다음 층으로 전파하는 것이 신경망 순전파의 흐름이다.\n",
    "\n",
    "역전파는 $dX = dY \\cdot W^T$이고, $dW = X^T \\cdot dY$, $dB = \\sum dY$이다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}W^T\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T\\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "\n",
    "$W^T$는 W의 전치행렬이다. 전치행렬은 W의 (i, j)원소를 (j, i)위치로 보내는 것이다.\n",
    "\n",
    "$$\n",
    "W = \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "W^T = \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{pmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2. 배치용 Affine 계층\n",
    "\n",
    "데이터 N개를 묶어 순전파하는 경우를 생각해보자.\n",
    "\n",
    "X의 형상은 (N, 2), W의 형상은 (2, 3)이다.\n",
    "\n",
    "X와 W의 내적은 N개의 데이터에 대한 내적을 모두 한꺼번에 계산한다.\n",
    "\n",
    "$$\n",
    "Y = XW + B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파\n",
    "import numpy as np\n",
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파\n",
    "import numpy as np\n",
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine 계층 구현\n",
    "class Affine:\n",
    "  def __init__(self, W, b):\n",
    "    self.W = W\n",
    "    self.b = B\n",
    "    self.x = None\n",
    "    self.dW = None\n",
    "    self.db = None\n",
    "    \n",
    "  def forward(self, x):\n",
    "    self.x = x\n",
    "    out = np.dot(x, self.W) + self.b\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = np.dot(dout, self.W.T)\n",
    "    self.dW = np.dot(self.x.T, dout)\n",
    "    self.db = np.sum(dout, axis=0)\n",
    "    return dx\n",
    "  \n",
    "\n",
    "x = np.random.rand(3, 2) # 입력\n",
    "W = np.random.rand(2, 3) # 가중치\n",
    "B = np.random.rand(3) # 편향\n",
    "\n",
    "affine = Affine(W, B)\n",
    "out = affine.forward(x)\n",
    "print(out)\n",
    "\n",
    "dout = np.random.rand(3)\n",
    "dx = affine.backward(dout)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3. Softmax-with-Loss 계층\n",
    "\n",
    "소프트맥스 함수는 입력 값을 정규화하여 출력한다.\n",
    "\n",
    "소프트맥스 함수의 출력을 교차 엔트로피 오차 함수로 정의하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^n \\exp(a_i)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "  c = np.max(a)\n",
    "  exp_a = np.exp(a - c) # 오버플로 대책\n",
    "  sum_exp_a = np.sum(exp_a)\n",
    "  y = exp_a / sum_exp_a\n",
    "  return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "  if y.ndim == 1:\n",
    "    t = t.reshape(1, t.size)\n",
    "    y = y.reshape(1, y.size)\n",
    "    \n",
    "  # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "  if t.size == y.size:\n",
    "    t = t.argmax(axis=1)\n",
    "    \n",
    "  batch_size = y.shape[0]\n",
    "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "  \"\"\"Softmax-with-Loss 계층\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self.loss = None # 손실\n",
    "    self.y = None # softmax의 출력\n",
    "    self.t = None # 정답 레이블(원-핫 벡터)\n",
    "    \n",
    "  def forward(self, x, t):\n",
    "    self.t = t\n",
    "    self.y = softmax(x)\n",
    "    self.loss = cross_entropy_error(self.y, self.t)\n",
    "    return self.loss\n",
    "  \n",
    "  def backward(self, dout=1):\n",
    "    batch_size = self.t.shape[0]\n",
    "    dx = (self.y - self.t) / batch_size\n",
    "    return dx\n",
    "  \n",
    "x = np.random.rand(10, 2) # 입력\n",
    "t = np.random.rand(10, 2) # 정답 레이블\n",
    "\n",
    "softmax_with_loss = SoftmaxWithLoss()\n",
    "loss = softmax_with_loss.forward(x, t)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. 오차역전파법 구현하기\n",
    "\n",
    "### 5.7.1. 신경망 학습의 전체 그림\n",
    "\n",
    "**전제**\n",
    "* 신경망에는 적응 가능한 가중치와 편향이 있다.\n",
    "* 가중치와 편향ㅇ르 훈련 데이터에 적응 하도록 조정하는 과정을 '학습' 이라고 한다.\n",
    "\n",
    "**1단계 - 미니배치**\n",
    "* 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라고 하며, 이 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\n",
    "* 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "**2단계 - 기울기 산출**\n",
    "* 미니배치의 손실 한수 값을 줄이기 위해 각 가중치 매개번수의 기울기를 구한다.\n",
    "* 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "**3단계 - 매개변수 갱신**\n",
    "* 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "**4단계 - 반복**\n",
    "* 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 오차적전파법을 적용한 신경망 구현하기\n",
    "\n",
    "2층 신경망 TwoLayerNet을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3. 오차역전파법으로 구한 기울기 검증하기\n",
    "\n",
    "수치 미분과 오차역전파법으로 구한 기울기를 비교하여 오차역전파법을 제대로 구현했는지 확인할 수 있다.\n",
    "\n",
    "수치 미분은 구현하기 쉽지만 계산 시간이 오래 걸린다.\n",
    "\n",
    "오차역전파법은 구현하기 복잡하지만, 기울기를 효율적으로 구할 수 있다.\n",
    "\n",
    "수치 미분의 결과와 오차역전파법의 결과를 비교하여 오차역전파법을 제대로 구현했는지 확인할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 확인\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: fixme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
