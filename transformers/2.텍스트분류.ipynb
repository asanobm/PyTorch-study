{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# !pip install 'transformers[torch]' datasets torch tensorflow sentencepiece numpy pandas -q\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.텍스트 분류 (Text Classification)\n",
    "\n",
    "* 텍스트 분류는 자연어 처리의 가장 대표적인 문제이다.\n",
    "* 텍스트 분류는 텍스트를 입력으로 받아, 텍스트가 어떤 종류의 범주에 속하는지를 구분하는 문제이다.\n",
    "  예를 들어 스팸 메일 분류기는 이메일을 입력으로 받아 스팸 메일인지 아닌지를 구분한다. 또한 영화 리뷰 분류기는 영화 리뷰를 입력으로 받아 리뷰가 긍정적인지 부정적인지를 구분한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*트랜스포머 모델을 훈련하는 전형적인 파이프라인*\n",
    "```mermaid\n",
    "graph LR\n",
    "    S1[데이터셋]\n",
    "    S2[토크나이저]\n",
    "    S3[트랜스포머스]\n",
    "    S4[데이터셋]\n",
    "\n",
    "    S1 -->|데이터셋 로드하고 전처리하기| S2\n",
    "    S2 -->|토크나이저로 토큰화하고 인코딩하기| S3\n",
    "    S3 -->|트랜스포머 모델 훈련하기| S4\n",
    "    S4 -->|훈련된 모델로 텍스트 분류하기| S5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 데이터셋\n",
    "\n",
    "영어로 작성된 트위터 메시지에 감정이 어떻게 표현되는지 분류하는 감정 분류기를 만든다.\n",
    "감정은 분노(angry), 혐오(disgust), 두려움(fear), 기쁨(joy), 슬픔(sadness), 놀람(surprise)의 여섯 개 감정으로 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 데이터셋 로드\n",
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(\"현재 허브에는 {}개의 데이터셋이 있습니다.\\n\".format(len(all_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion 데이터셋 로드\n",
    "from datasets import load_dataset\n",
    "emotions = load_dataset('emotion')\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = emotions['train']\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 개의 행이 하나의 딕셔너리로 구성되어 있다. 키는 'label'과 'text'이다.\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키를 이용해 값을 가져올 수 있다.\n",
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. 데이터셋에서 데이터프레임 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions.set_format('pandas')\n",
    "df = emotions['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 숫자에서 문자로 변환\n",
    "def label_int2str(row):\n",
    "  return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "# 레이블명 컬럼 추가\n",
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. 클래스 분포 확인\n",
    "\n",
    "* 텍스트 분류 문제를 풀 때는 데이터셋의 클래스 분포를 확인하는 것이 중요하다.\n",
    "* 클래스 분포가 평향된 데이터셋은 훈련 손실과 평가 지표 측면에서 균형 잡힌 데이터셋과 다른 처리법이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "# with label counts\n",
    "for index, value in enumerate(df['label_name'].value_counts(ascending=True)):\n",
    "  plt.text(value, index, str(value))\n",
    "plt.title(\"Frequency of classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터셋은 불균형이 심하다. joy와 sadness 클래스가 ㅁ낳이 등장하고 love와 surprise 클래스는 5~10배 정도 적게 등장한다.\n",
    "\n",
    "불균형한 데이터는 다음 방법을 사용해 균형을 맞출 수 있다.\n",
    "\n",
    "* 소수 클래스를 랜덤하게 오버샘플링 한다.\n",
    "* 다수 클래스를 랜덤하게 언더샘플링 한다.\n",
    "* 클래스의 대표성이 부족하다면 레이블된 데이터를 더 많이 수집한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. 트위 길이 확인\n",
    "\n",
    "* 트랜스포머 모델은 *최대 문맥 크기(maximum context size)*라는 최대 입력 시퀀스 길이가 있다.\n",
    "* DistilBERT 모델의 최대 문맥 크키는 512토큰으로 문단 몇개 정도가 된다.\n",
    "* 트윗당 단어 분포를 보면 감정에 따른 트위 길이가 대략 추정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words per tweet'] = df['text'].str.split().apply(len)\n",
    "df.boxplot('words per tweet', by='label_name', grid=False, showfliers=False, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 감정의 트윗 길이는 15개 단어 정도이다.\n",
    "* 가장 긴 트윗도 DistilBERT의 최대 문맥 크기보다 더 작다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.텍스트에서 토큰으로\n",
    "\n",
    "* DistilBERT 같은 트랜스포머 모델은 원시 문자열을 입력으로 받지 못한다.\n",
    "\n",
    "**토큰화 방법**\n",
    "\n",
    "1. 문자 토큰화\n",
    "2. 단어 토큰화\n",
    "3. 부분단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. 문자 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치화\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전에 있는 각 문자를 고유한 정수로 바꾸는 매핑 딕셔너리\n",
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 토큰이 고유한 숫자 식별자로 매핑된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = pd.DataFrame({\"Name\":[\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0, 1, 2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 핫 백터로 변환\n",
    "pd.get_dummies(categorical_df, columns=[\"Label ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**하나의 항목만 1이고 나머지는 0인 벡터로 변환**을 원-핫 벡터라고 한다.\n",
    "벡터의 두 원소가 1이면 해당하는 토큰이 동시에 존재한다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch input_ids를 텐서로 바꾸고 one_hot()함수를 이용해 원 핫 인코딩을 만든다.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38개의 입력 토큰 각각에 20 차원의 원-핫 벡터가 만들어 진다.\n",
    "\n",
    "어휘사전을 구성하는 고유한 문자가 20개이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"토큰: {tokenized_text[0]}\")\n",
    "print(f\"텐서 인덱스: {input_ids[0]}\")\n",
    "print(f\"원 핫 인코딩: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. 단어 토큰화\n",
    "\n",
    "* 텍스트를 문자가 아니라 단어로 분할하고 각 단어를 정수로 매핑한다.\n",
    "* 단어 토큰화는 처음부터 단어를 사용하기 때문에 문자에서 단어를 학습하는 단계가 생략되어 훈련 과정의 복잡도가 감소한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 수준 토큰화에서 수행한 단계를 동일하게 적용해 각 단어를 하나의 ID에 매핑한다.\n",
    "**구두점을 제거하고 모든 문자를 소문자로 변환**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. 부분단어 토큰화\n",
    "\n",
    "기본적으로 문자 토큰화와 단어 토큰화의 장점을 결합한 형태이다. 드믈게 등장하는 단어를 더 작은 단위로 나누면 모델이 복잡한 단어나 철차 오류를 처리하기 용이하다.\n",
    "또한 입력 길이를 적절한 크기로 유지하기 위해 자주 등장하는 단어를 고유한 항목으로 유지한다. 부분단어 토큰화는 통계 규칙과 알고리즘 을 함께 사용해 사전 훈련 말뭉치에서 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer클래스는 체크포인트 이름을 사용해 모델의 설정, 사전 훈련된 가중치, 어휘사전을 자동으로 추출하는 자동 클래스 이다.\n",
    "# 이 클래스를 사용하면 모델 간의 빠른 전환이 가능하지만, 특정 클래스를 수동으로 로드할 수도 있다.\n",
    "\n",
    "# DisstilBERT 토크나이저 로드\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 토큰화처럼 단어가 input_ids 필드에 있는 고유한 정수에 매핑된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text['input_ids'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [CLS], [SEP]가 시퀀스 처음과 끝에 추가된다.\n",
    "  이런 토큰은 모델마다 다르며, 시퀀스의 시작과 끝을 나타낸다.\n",
    "* 모든 토큰이 소문자로 변환된다.\n",
    "* `tokenizing`, `NLP`가 각각 2개의 토큰으로 분할된다.\n",
    "  자주 등장하는 단어가 아니기 때문에 `###izing`, `##p`에 있는 #는 앞의 문자열이 공백이 아님을 나타낸다.\n",
    "  이런 접두사가 붙은 토큰은 문자열로 다시 바꿀 때 앞의 토큰과 합쳐진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전의 크기 확인\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 최대 문맥 크기 확인\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. 전체 데이터셋 토큰화\n",
    "\n",
    "전체 말뭉치를 토큰화하기 위해서`DatasetDict`객채의 `map`메서드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "  # padding=True 로 설정하면 배치에 있는 가장 긴 샘플 크기에 맞춰 심플을 0으로 패딩한다.\n",
    "  # truncation=True 로 설정하면 모델의 최대 문맥 크기에 맞춰 샘플을 잘라낸다.\n",
    "  return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# 확인\n",
    "\n",
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 문장이 두번째 문자보다 길이가 짧다. 이런 경우에는 두번째 문장을 첫번째 문장의 길이에 맞추기 위해 패딩을 추가한다.\n",
    "여기서 패딩은 0으로 채워진 토큰이다. 어휘사전에는 [PAD]에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "print(emotions_encoded['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 텍스트 분류 모델 훈련하기\n",
    "\n",
    "DistilBERT 같은 모델은 텍스트 시퀀스에 있는 마스킹된 단어를 예측하도록 사전 훈련된다. 하지만 이런 언어 모델은 텍스트 분류에 바로 사용하지 못한다. \n",
    "\n",
    "DistilBERT 같은 인코더 기반 모델의 구조\n",
    "\n",
    "1. 텍스트를 토큰화해 토큰 인코딩이라 부르는 `원-핫 벡터`로 변환한다.\n",
    "2. 토큰 인코딩을 저차원 공간의 벡터인 `토큰 임베딩`으로 변환한다.\n",
    "3. 토큰 임베딩을 `인코더 블록`층에 통과시켜 각 입력 토큰에 대한 은닉 상태를 만든다.\n",
    "4. 각 은틱 상태는 언어 모델링의 사전 훈련 목표를 달성하기 위해 입력 토큰을 예측하는 층으로 전달된다.\n",
    "5. 분류 작업에서는 이 언어 모델링층을 분류층으로 변환한다.\n",
    "\n",
    "*특성추출*\n",
    "\n",
    "사전 훈련된 모델을 수정하지 않고 은닉 상태를 특성으로 사용해 분류모델을 훈련\n",
    "\n",
    "*미세튜닝*\n",
    "\n",
    "사전 훈련된 모델의 파라미터도 업데이트하기 위해 전체 모델을 `엔드-투-엔드`로 훈련한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. 트랜스포머를 특성 추출기로 사용하기\n",
    "\n",
    "트랜스포머는 특성 추출기로 사용하기 편하다. 트랜스포머는 훈련하는 동안 바디의 가중치를 동결하고 은닉 상태를 분류 모델의 특성으로 사용한다.\n",
    "\n",
    "이 방식은 작거나 얕은 모델을 빠르게 훈련한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사전 훈련된 모델 사용하기**\n",
    "\n",
    "트랜스포머의 `AutoModel`은 `AutoTokenizer`와 비슷하게 사전 훈련된 가중치를 로드하는 `from_pretrained`메서드를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "device = get_device()\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**마지막 은닉 상태 추출하기**\n",
    "\n",
    "문자열을 인코딩하고 토큰을 파이토치 텐서로 변환한다. `return_tensors=pt` 매개변수를 지정해 텐서를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tis is a text\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"입력 텐서 크기 : {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "  outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉 상태 텐서의 크기는 [batch_size, n_nokens, hidden_dim]이다. \n",
    "\n",
    "6개의 입력 토큰마다 768차원의 벡터를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 은닉 상태를 얻는 방법\n",
    "outputs.last_hidden_state[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "  # 모델 입력을 GPU로 이동\n",
    "  inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "  # 마지막 은닉 상태를 얻는다.\n",
    "  with torch.no_grad():\n",
    "    last_hidden_state = model(**inputs).last_hidden_state\n",
    "  # [CLS]토큰에 대한 벡터를 반환한다.\n",
    "  return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 모델은 텐서가 입력되기를 기대하므로 input_ids와 attention_mask 열을 \"torch\" 포멧으로 변경한다.\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 분할에 대해 은닉 상태를 한번에 추출한다.\n",
    "# batch_size=None으로 지정하지 않았기 때문에 기본값 1000이 사용된다.\n",
    "# 기대한 대로 extract_hidden_states() 함수를 적용하면 새로운 hidden_state 열이 데이터셋에 추가된다.\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**특성 행렬 마들기**\n",
    "\n",
    "전처리된 데이터셋에 분류 모델을 훈련하는데 필요한 모든 정보가 담겨있다.\n",
    "\n",
    "은닉 상태를 입력 특성으로 사용하고 레이블을 타깃으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉 상태로 모델을 훈련하기 전에 분류하려는 감저엥 대한 융여한 표현을 제공하는지 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련 세트 시각화하기**\n",
    "\n",
    "768차원의 은닉상태를 시각화하기는 어렵기 때문에 UMAP 알고리즘을 사용해서 이 벡터를 2차원으로 줄인다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 특성 스케일을 [0, 1]로 조정한다.\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# UMAP 객체를 생성하고 훈련한다.\n",
    "mapper = UMAP(n_components=2, metric='cosine', random_state=42).fit(X_scaled)\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=['X', 'Y'])\n",
    "df_emb['label'] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 결과는 훈련 샘플과 개수가 동일한 배열이다. 하지만 특성은 768개가 아니라 2개 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "  df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "  axes[i].hexbin(df_emb_sub['X'], df_emb_sub['Y'], cmap=cmap, gridsize=20, linewidths=(0,))\n",
    "  axes[i].set_title(label)\n",
    "  axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**간단한 분류 모델 훈련하기**\n",
    "\n",
    "은닉 상태가 감정별로 조금씩 다르지만 일부 감정 사이에는 명확한 경계가 없기도 하다.\n",
    "\n",
    "이 은닉 상태를 사용해 사이킷런의 로지스틱 회귀 모델을 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 수렴을 보장하기 위해 `max_iter`를 증가 시킨다.\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 DummyClassifier와 비교해 보면 DistilBERT 임베딩을 사용한 간단한 분류 모델은 기준 모델보다 훨씬 높은 정확도를 달성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**오차 행렬 시각화하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "  fig, ax = plt.subplots(figsize=(6,6))\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "  plt.title(\"Normalized confusion matrix\")\n",
    "  plt.show()\n",
    "\n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`anger`와 `fear`는 `sadness`와 가장 많이 혼동된다.\n",
    "\n",
    "`love`와 `surprise`는 `joy`로 많이 오인된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 트랜스포머 미세 튜닝하기\n",
    "\n",
    "미세 튜닝 방식에서는 은닉 상태를 고정된 특성으로 사용하지 않고 전체 모델을 훈련한다.\n",
    "\n",
    "이를 위해 분류 헤드는 미분이 가능해야한다. 따라서 이 방식은 신경망으로 분류 작업을 수행한다.\n",
    "\n",
    "분류 모델에 입력으로 사용하는 은닉 상태를 훈련하면 분류 작업에 적합하지 않은 데이터를 다룰다는 문제를 회피할 수 있다.\n",
    "\n",
    "초기 은식 상태는 훈련하는 동안에 모델 손실이 감소하도록 수정되고 성능이 높아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사전 훈련된 모델 로드하기**\n",
    "\n",
    "1. 사전 훈련된 DistilBERT 모델\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 일부가 랜덤하게 초기화된다는 경고가 발생한다. 이 경고는 모델의 일부가 사전 훈련된 가중치로 초기화되지 않았다는 것을 의미한다.\n",
    "정상적인 동작이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 훈련하기**\n",
    "\n",
    "훈련 파라미터를 정의하기 위해 `TrainingArguments`클래스를 사용한다. 이 클래스는 많은 정보를 저장하며 훈련과 평가를 상세하게 제어할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "print(f\"모델 이름: {model_name}\")\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True,\n",
    "                                  hub_private_repo=True,\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기, 학습률, 에포크 횟수도 설정하고 훈련이 끝난 후 최상의 모델을 로드하도록 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 케라스로 미세 튜닝하기\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "print(f\"Model Check Point: {model_ckpt}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "tf_model = (TFAutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels, from_pt=True))\n",
    "\n",
    "tokenizer_columns = tokenizer.model_input_names\n",
    "\n",
    "tf_train_dataset = emotions_encoded[\"train\"].to_tf_dataset(\n",
    "  columns=tokenizer_columns,\n",
    "  label_cols=[\"label\"],\n",
    "  shuffle=True,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "tf_eval_dataset = emotions_encoded[\"validation\"].to_tf_dataset(\n",
    "  columns=tokenizer_columns,\n",
    "  label_cols=[\"label\"],\n",
    "  shuffle=False,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "tf_model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**오류 분석**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "  # 모든 입력 텐서를 모델과 같은 장치로 이동\n",
    "  inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "  with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "    pred_label = torch.argmax(output.logits, axis=-1)\n",
    "    loss = cross_entropy(output.logits, batch[\"label\"].to(device), reduction=\"none\")\n",
    "    # 다른 데이터셋 열과 호환되도록 텐서를 CPU로 이동\n",
    "    return {\"loss\": loss.cpu(), \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 샘플의 손실을 구한다.\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# 손실 값을 계산한다.\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트, 손실, 예측 레이블과 진짜 레이블DataFrame을 만든다.\n",
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].apply(label_int2str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***잘못된 레이블***\n",
    "\n",
    "* 데이터에 레이블을 부여하는 프로세스는 모두 완벽하지 않다.\n",
    "* 레이블을 부여하는 애너데이터가 실수할 수 있고, 레이블을 부여하는 방법에 따라 레이블이 달라질 수 있다.\n",
    "* 잘못된 레이블이 존재하는 것은 자연스럽다.\n",
    "\n",
    "***데이터셋의 특이사항***\n",
    "\n",
    "* 데이터셋에는 특이사항이 존재할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values('loss', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values('loss', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 저장 및 공유**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = 'asanobm/distilbert-base-uncased-finetuned-emotion'\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"I saw a movie today and it was realy good.\"\n",
    "\n",
    "preds = classifier(tweet, return_all_scores=True)\n",
    "\n",
    "preds_df = pd.DataFrame(preds[0])\n",
    "plt.bar(labels, 100 * preds_df['score'], color='C0')\n",
    "plt.title(f\"{tweet}\")\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
